{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Scraper\n",
    "Our goal in this notebook is to scrape data for house sales in the city of Thesaloniki. We will scrape data from the biggest Greek website for house sales: spitogatos.gr. First we import all the necessary libraries. We are going to use selenium for the scraping.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import numpy as np\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.common.exceptions import StaleElementReferenceException\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.common.exceptions import WebDriverException\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions\n",
    "import time\n",
    "import datetime\n",
    "import logging\n",
    "from scrape import many_page_scrape,get_driver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to know what a WebDriver element is. A WebDriver element is a Python object representing an HTML element on a web page, as found and controlled by Selenium WebDriver. It allows you to interact with elements on the page—such as clicking buttons, entering text, or reading content—using Python code.As a start we initialize a webdriver element. When we run this a chromium window should open. Maximize the window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us manually go now the desired website spitogatos.gr. Notice that the website requires us to solve a captcha before we can access the listings. Throughout this script we will be required to manually solve the captcha every now and then since the website has captchas that appear if suspected bot activity is detected. First let us explore this webpage a bit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ![Alt text](Screenshot.png)
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Searching for houses in Thessaloniki, Greece we get:  ![Alt text](Screenshot3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what we want is to scrape the HTML code of this website. Notice that there are 30 entries in each page and there are more than a thousand pages in total. For that reason we have created the scrape.py file that cotains all the necessary functions that will be used in the scraping process. These function were built using [selenium](https://www.selenium.dev/documentation/webdriver/) and following the relevant documentation. For more details take a look at the scrape.py file.![Alt text](Screenshot22.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the houses are all displayed without any filters or attributes besides the basics that we see on the entries like floor, number of rooms, number of bathrooms, price and total area. So at first glance these are the only attributes that we are able to scrape without going into much trouble. \n",
    "\n",
    "In order for us to gather more information like year the house was built and other characteristics we will need to be more creative. We have created a function in the scrape.py file, called many_page_scrape,  which basically takes a url and starts scraping all the available pages for that url. So in order to find the year each house was bulit we will put a filter and display only houses built 1952. Once our function scrapes all those entries we will continue with 1953, 1954 and so on until 2024. The script will save these data as tables with columns floor, number of rooms, price, total area, location, year of construction. \n",
    "\n",
    "\n",
    "After we scrape all years we will similarly apply all other filters and get different tables with the same entries. After cross examining all the dofferent tables we can construct a final file which will contain detailed data for all the houses.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with the scraping process first. Notice that if we wanted to scrape a different location than Thessaloniki we would have to change the urls in the scrape.py file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following script performs the scraping process  by attribute, with folder management and captcha handling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "**Parameters:**\n",
    "- `folder_url` (str): The path to the main folder where the data will be saved.\n",
    "- `attribute` (str): The attribute by which to filter house listings (e.g., \"autonomous_heating\"). \n",
    "\n",
    "- `start_at_first_page` (bool, optional): If `True`, initializes the driver to start at the first page with the given attribute filter. Default is `False`.\n",
    "\n",
    "**Workflow:**\n",
    "1. Optionally initializes the Selenium driver to the first page with the specified attribute.\n",
    "2. Creates the main data folder and a subfolder for the specific attribute, handling any errors if folders already exist.\n",
    "3. Prints the attribute being scraped.\n",
    "4. Initializes an empty DataFrame with columns for house details.\n",
    "5. Calls `many_page_scrape` to scrape all pages for the specified attribute, handling captchas by prompting the user to solve them manually if a `TimeoutException` occurs.\n",
    "\n",
    "**Note:**  \n",
    "- The function expects the global `driver` object and the `many_page_scrape` and `get_driver` functions to be defined and imported.\n",
    "- Data for each attribute is saved in its corresponding subfolder under the main folder.\n",
    "- The scraped data will be saved in batches of 50 pages (so around 1500 entries) each. The last batch will be usually smaller than that and it will have a different name. \n",
    "- We note that the script is built in such a way that it removes duplicate entries before saving them. That was necessary because spitogatos.gr often contains duplicate house ads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def main(folder_url,attribute,start_at_first_page=True):\n",
    "\n",
    "    if start_at_first_page:\n",
    "        get_driver(driver,attribute=attribute)\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        os.mkdir(folder_url)\n",
    "        \n",
    "        print(f\"Folder '{folder_url}' created successfully.\")\n",
    "    except FileExistsError:\n",
    "        print(f\"Folder '{folder_url}' already exists.\")\n",
    "    except OSError as error:\n",
    "        print(f\"Error creating folder '{folder_url}': {error}\")\n",
    "    try:\n",
    "        os.mkdir(f\"{folder_url}/Houses_data_{attribute}\")\n",
    "        print(f\"Folder Houses_data_{attribute} created successfully.\")\n",
    "    except FileExistsError:\n",
    "        print(f\"Folder Houses_data_{attribute} already exists.\")\n",
    "    except OSError as error:\n",
    "        print(f\"Error creating folder Houses_data_{attribute}: {error}\")\n",
    "\n",
    "    print(f\"We are scraping attribute {attribute}\")\n",
    "\n",
    "    df=pd.DataFrame(columns=[\"Location\", \"Price\",\"Total_area\",\"House_type\",\"Floor\",\"Rooms\",\"Bathrooms\",\"submission_date\"])\n",
    "    while True:\n",
    "        try:\n",
    "            df = many_page_scrape(driver, df, attribute=attribute)\n",
    "            break  # Exit loop if successful\n",
    "        except TimeoutException:\n",
    "            input(\"TimeoutException: Please solve the captcha in the browser, then press Enter to retry...\")\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The attribute variable can be any of the following: \n",
    "\n",
    "['autonomous_heating', 'central_heating', 'individual_heating', 'no_heating', 'petrol_heating', 'natural_gas_heating', 'LPG_heating', 'electrical_heating', 'thermal_storage_heating', 'wood_headting', 'pellet_heating', 'heat_pump_heating', 'with_AC', 'with_storage_room', 'with_elavator', 'with_solar_heater', 'with_fireplace', 'Furnished', 'with_parking', 'with_garden', 'with_pool', 'with_balcony', 'last_floor', 1952, 1953, 1954, 1955, 1956, 1957, 1958, 1959, 1960, 1961, 1962, 1963, 1964, 1965, 1966, 1967, 1968, 1969, 1970, 1971, 1972, 1973, 1974, 1975, 1976, 1977, 1978, 1979, 1980, 1981, 1982, 1983, 1984, 1985, 1986, 1987, 1988, 1989, 1990, 1991, 1992, 1993, 1994, 1995, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2024]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_name =f\"/home/tsantaris/OneDrive/Data science and AI stuff/Project spitogatos/Houses_data_{datetime.date.today()}\"\n",
    "\n",
    "main(folder_name,2007,start_at_first_page=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spitogatos_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
